# -*- coding: utf-8 -*-
"""Bert Process.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zvFkpgU2E02QQ9sryQU0rVlBNcYP4Z1k
"""

#!pip install tensorflow_text

import math
import numpy as np
import pandas as pd
import csv
import tensorflow as tf
import tensorflow.keras
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense
import tensorflow_hub as hub
import tensorflow_text as text
import random

df_list = []

def label_type(row):
    #print(row['Type'])
    if row['Type'] == 'Conversation' :
      #print(0)
      return int(0)
    if row['Type'] == 'Intruction' :
      #print(1)
      return int(1)
    if row['Type'] == 'Strategy' :
      #print(2)
      return int(2)
    else:
      return int(3)

for i in range(1, 29):
    df = pd.read_csv('Data/Data '+str(i)+'.csv', sep=',')
    df['Num Type'] = df.apply(lambda row: label_type(row), axis=1)
    df_list.append(df)
    
data_indices = list(range(28))
random.shuffle(data_indices)

train_indices = data_indices[:14]
tuning_indices = data_indices[14:21]
testing_indices = data_indices[21:]

train_x = []
tuning_x = []
test_x = []

train_y = []
tuning_y = []
test_y = []

for i in train_indices:
    #print(i)
    df = df_list[i]
    for j in range(len(df)):
        #if(type(df['Utterance'][j]) == str and type(df['Type'][j]) == str):
        if(df['Num Type'][j] != 3):
          train_x.append(df['Utterance'][j])
          one_hot_label = [0, 0, 0]
          one_hot_index = int(df['Num Type'][j])
          one_hot_label[one_hot_index] = 1
          train_y.append(one_hot_label)
          #train_y.append(int(df['Num Type'][j]))
    

for i in tuning_indices:
    #print(i)
    df = df_list[i]
    for j in range(len(df)):
        #if(type(df['Utterance'][j]) == str and type(df['Type'][j]) == str):
        if(df['Num Type'][j] != 3):
          tuning_x.append(df['Utterance'][j])
          one_hot_label = [0, 0, 0]
          one_hot_index = int(df['Num Type'][j])
          one_hot_label[one_hot_index] = 1
          tuning_y.append(one_hot_label)
          #tuning_y.append(int(df['Num Type'][j]))
    
for i in testing_indices:
    #print(i)
    df = df_list[i]
    for j in range(len(df)):
        #if(type(df['Utterance'][j]) == str and type(df['Type'][j]) == str):
        if(df['Num Type'][j] != 3):
          test_x.append(df['Utterance'][j])
          one_hot_label = [0, 0, 0]
          one_hot_index = int(df['Num Type'][j])
          one_hot_label[one_hot_index] = 1
          test_y.append(one_hot_label)
          #test_y.append(int(df['Num Type'][j]))

#print(train_y)
#print(np.array(train_y).shape)

bert_preprocess_model = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')
bert_model = hub.KerasLayer('https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1')

encoder_inputs = bert_preprocess_model(train_x)
outputs = bert_model(encoder_inputs)
pooled_output = outputs["pooled_output"]
sequence_output = outputs["sequence_output"]

print(pooled_output)
#print(sequence_output)

model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(40, activation='sigmoid'))
model.add(tf.keras.layers.Dense(20, activation='sigmoid'))
model.add(tf.keras.layers.Dense(3, activation='softmax'))
model.compile(loss='CategoricalCrossentropy', optimizer='adam', metrics=['CategoricalAccuracy', 'CategoricalCrossentropy'])

y_pred = model.fit(pooled_output, np.array(train_y), epochs=20, batch_size=20)

print(y_pred)
tf.keras.utils.plot_model(model)

encoder_tuning = bert_preprocess_model(tuning_x)
outputs_tuning = bert_model(encoder_tuning)
pooled_output_tuning = outputs_tuning["pooled_output"]

y_tuning_pred = model.predict(pooled_output_tuning)
score_pred = tf.keras.losses.CategoricalCrossentropy()

print(y_tuning_pred)
print('\nTuning Categorical Cross Entropy:')
score_pred(tuning_y, y_tuning_pred).numpy()

def probability_class_assignment(vec):
  for i in range(len(vec)):
    if vec[i][0] > vec[i][1] and vec[i][0] > vec[i][2]:
      vec[i][0] = 1
      vec[i][1] = 0
      vec[i][2] = 0
    elif vec[i][1] > vec[i][2]:
      vec[i][0] = 0
      vec[i][1] = 1
      vec[i][2] = 0
    else:
      vec[i][0] = 0
      vec[i][1] = 0
      vec[i][2] = 1
  return np.array(vec)

y_tuning_pred_one_hot = probability_class_assignment(y_tuning_pred)
print(y_tuning_pred)

tuning_counts = np.array(tuning_y).sum(axis=0)
tuning_counts_pred = y_tuning_pred_one_hot.sum(axis=0)

print('Original Count of Conversational Utterances:',tuning_counts[0])
print('Predicted Count of Conversational Utterances:',tuning_counts_pred[0])

print("Original Count of Instructional Utterances:",tuning_counts[1])
print("Predicted Count of Instructional Utterances:",tuning_counts_pred[1])

print("Original Count of Strategy Utterances",tuning_counts[2])
print("Predicted Count of Strategy Utterances",tuning_counts_pred[2])

#Test Set
encoder_test = bert_preprocess_model(test_x)
outputs_test = bert_model(encoder_test)
pooled_output_test = outputs_test["pooled_output"]

y_test_pred = model.predict(pooled_output_test)

print(y_test_pred)
print('\nTuning Categorical Cross Entropy:')
score_pred(test_y, y_test_pred).numpy()

y_test_pred_one_hot = probability_class_assignment(y_test_pred)
print(y_test_pred)

test_counts = np.array(test_y).sum(axis=0)
test_counts_pred = y_test_pred_one_hot.sum(axis=0)

print('Original Count of Conversational Utterances:',test_counts[0])
print('Predicted Count of Conversational Utterances:',test_counts_pred[0])

print("Original Count of Instructional Utterances:",test_counts[1])
print("Predicted Count of Instructional Utterances:",test_counts_pred[1])

print("Original Count of Strategy Utterances",test_counts[2])
print("Predicted Count of Strategy Utterances",test_counts_pred[2])
